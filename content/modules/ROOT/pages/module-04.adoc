[4. Affinity and Anti-Affinity for VM Placement ]

= Affinity and Anti-Affinity for VM Placement

This lab is designed to enhance your proficiency in applying Node Affinity, Pod Affinity, and Anti-Pod Affinity specifically to Virtual Machines. The core purpose is to illustrate the practical application of these principles in real-world scenarios and solidify your conceptual understanding.


[#nodeaffinity]
== Node Affinity

Node Affinity defines rules that guide the scheduler to attract a Virtual Machine Pod to a specific node or group of nodes. These rules rely on matching labels applied to the nodes.

The core use case for Node Affinity is to ensure that a VM runs only on nodes that possess specific hardware features, such as a particular GPU model or a high amount of RAM, by matching the corresponding labels on the node.

This lab will demonstrate how Node Affinity is set up and how it functions.

[start=1]
. First navigate to the affinity project.
Home → Project → affinity

[start=2]
. Login into your bastion host with credentials provided to you by the lab administrator. You will need this for later steps. You will need this for later steps.

[source,bash]
----	
ssh lab-user@<host> -p 32270
----

[start=3]
. Using the credentials provided by the lab administrator, log in to the OpenShift CLI from your terminal on the bastion host. Be sure to substitute the placeholder password and cluster name with the specific values you were given.
[source,bash]
----
oc login -u admin -p <password> --server=https://api.cluster-<clustername>.dynamic.redhatworkshops.io:6443
----

[start=4]
. Find out what node the node-affinity-vm is running on. 
To do this you can look at the vm in the affinity project or you can use the openshift CLI.

[source,bash]
----
oc project affinity
oc get vmi node-affinity-vm
----

[start=5]
. Set an east node label on any work where the node-affinity-vm is currently not running.

[source,bash]
----
oc get nodes
oc label node <worker where node-affinity-vm is not running> zone=east
----

Alternatively you can use the OpenShift Console by going to Compute → Nodes and then pick a node where node-affinity-vm is not running. Then click the 3 dots and click edit labels.

image::exercise4/04-image-affinity.png[title="Label Nodes", link=self, window=blank, width=100%]


[start=6]
. Make sure the label was applied.
[source,bash]
----
oc get nodes <worker you just labeled> --show-labels | grep -i zone=east
----

[start=7]
. Navigate to Virtualization → Virtual Machines → node-affinity-vm → Configuration 

image::exercise4/04-image-affinity01.png[title="Node Affinity Navigation", link=self, window=blank, width=100%]


[start=8]
. Now navigate to Scheduling → Affinity rules
image::exercise4/04-image-affinity02.png[title="Node Affinity add rule", link=self, window=blank, width=100%]

[start=9]
. Click add affinity rule

[start=10]
. Change the condition to preferred during scheduling and set the weight to 75.
. Under Node Labels click Add Expression. Make the key zone and the values east. Your Final Node Affinity rule will look like the picture below. Confirm this is true and click save affinity rule.

image::exercise4/04-image-affinity03.png[title="Node Affinity Rule", link=self, window=blank, width=100%]

[start=11]
. Then click apply rules.

[start=12]
. Now let's take a look at the Node Affinity rule on node-affinity-vm. You can do this by using the GUI and going to YAML on the pod-anti-affinity-vm or you can do it on the CLI.

[source,bash]
----
oc edit vm pod-anti-affinity-vm 
----

[source,bash]
----
:q!
----

[start=13]
. To make the vm follow these new affinity rules. You normally would migrate it to the node with the label. However, to show that this affinity rule is working. We are going to power off the vm completely and then power it back on. 

[start=14]
. Click Action stop.

[start=15]
. Once the VM completely stops. Click Actions Start.

[start=16]
. The VM's relocation to the node with the assigned label is visible via the GUI. Alternatively, you can verify this change using the OCP CLI.
[source,bash]
----
oc get vmi node-affinity-vm
---- 

[#podaffinity]
== Pod Affinity
Pod Affinity is a scheduling rule that co-locates a VM Pod and an existing Pod (or another VM Pod) with a specific label onto the same node.

The primary benefit of using pod affinity is to improve performance for dependent VMs or services that require low-latency communication by guaranteeing their placement on the same worker node.

This lab will illustrate the setup and function of Pod Affinity.

[start=1]
. To begin, we need to apply a label to the node-affinity-vm. This will enable it to serve as the VM that our pod will have pod affinity to. The label can be added by editing the VM within the OpenShift CLI and including app: node-affinity-vm
[source,bash]
----
oc edit vmi node-affinity-vm -n affinity
----

[source,bash]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstance
metadata:
  annotations:
    kubevirt.io/latest-observed-api-version: v1
    kubevirt.io/storage-observed-api-version: v1
    kubevirt.io/vm-generation: "3"
    vm.kubevirt.io/flavor: small
    vm.kubevirt.io/os: rhel9
    vm.kubevirt.io/workload: server
  creationTimestamp: "2025-11-13T13:25:16Z"
  finalizers:
  - kubevirt.io/virtualMachineControllerFinalize
  - foregroundDeleteVirtualMachine
  generation: 13
  labels:
    app: node-affinity-vm  ←—--
    kubevirt.io/domain: node-affinity-vm
    kubevirt.io/nodeName: worker-cluster-t96sv-1
    kubevirt.io/size: small
    network.kubevirt.io/headlessService: headless
----

[source,bash]
----
:wq!
----

[source,bash]
----
oc edit vm node-affinity-vm -n affinity
----

[source,bash]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  annotations:
    kubemacpool.io/transaction-timestamp: "2025-11-12T05:17:34.117234412Z"
    kubevirt.io/latest-observed-api-version: v1
    kubevirt.io/storage-observed-api-version: v1
    vm.kubevirt.io/validations: |
      [
        {
          "name": "minimal-required-memory",
          "path": "jsonpath::.spec.domain.memory.guest",
          "rule": "integer",
          "message": "This VM requires more memory.",
          "min": 1610612736
        }
      ]
  creationTimestamp: "2025-11-12T02:02:05Z"
  finalizers:
  - kubevirt.io/virtualMachineControllerFinalize
  generation: 3
  labels:
    app: node-affinity-vm  ←—- Not here
    kubevirt.io/dynamic-credentials-support: "true"
    podaffinity: ""
    vm.kubevirt.io/template: rhel9-server-small
    vm.kubevirt.io/template.namespace: openshift
    vm.kubevirt.io/template.revision: "1"
    vm.kubevirt.io/template.version: v0.34.1
  name: node-affinity-vm
  namespace: affinity
  resourceVersion: "1485819"
  uid: 20136774-16cb-46e5-8cb4-417de1712da6
spec:
  dataVolumeTemplates:
  - apiVersion: cdi.kubevirt.io/v1beta1
    kind: DataVolume
    metadata:
      creationTimestamp: null
      name: node-affinity-vm
    spec:
      sourceRef:
        kind: DataSource
        name: rhel9
        namespace: openshift-virtualization-os-images
      storage:
        resources:
          requests:
            storage: 30Gi
  runStrategy: RerunOnFailure
  template:
    metadata:
      annotations:
        vm.kubevirt.io/flavor: small
        vm.kubevirt.io/os: rhel9
        vm.kubevirt.io/workload: server
      creationTimestamp: null
      labels:
        app: node-affinity-vm  ←—- insert it here
        kubevirt.io/domain: node-affinity-vm
        kubevirt.io/size: small
        network.kubevirt.io/headlessService: headless
----

[source,bash]
----
:wq!
----

[start=2]
. Click the pod-affinity-vm  

[start=3]
. See where the pod-affinity-vm is running by looking at OpenShift Console or check on the OpenShift Command line. 

[source,bash]
----
oc get vmi pod-affinity-vm
----

[start=4]
. Click Configure → scheduling

[start=5]
. Click the Affinity rules

image::exercise4/04-image-affinity04.png[title="Pod Affinity Rule", link=self, window=blank, width=100%]

[start=6]
. Add Affinity rule

[start=7]
. Change the type to  Workload (pod) Affinity

[start=8]
. Keep the condition to Required during scheduling.

[start=9]
. Keep the Topology key they same 

[start=10]
. Click Add expression under Workload labels

[start=11]
. We are going to now set a key so this vm will run on the same node as node-affinity-vm. Set the key to app and values to node-affinity-vm. Your Final Pody Affinity rule will look like the picture below. Confirm this is true and click Save affinity rule.

image::exercise4/04-image-affinity05.png[title="Pod Affinity Rule", link=self, window=blank, width=100%]

[start=12]
. Click Apply Rules

[start=13]
. Now let's take a look at the Pod Affinity rule on pod-affinity-vm. You can do this by using the GUI and going to YAML on the pod-anti-affinity-vm or you can do it on the CLI.

[source,bash]
----
oc edit vm pod-anti-affinity-vm 
---- 

[source,bash]
----
:q!
----

[start=14]
. See where the pod-affinity-vm and node-affinity-vm are running by looking at OpenShift Console or check on the OpenShift Command line. 

[source,bash]
----
oc get vmi pod-affinity-vm
oc get vmi node-affinity-vm
----

[start=15]
. To make the vm follow these new affinity rules. You normally would migrate it to the node with the label. However, to show that this affinity rule is working. We are going to power off the vm completely and then power it back on. 

[start=16]
. On Click Action stop.

[start=17]
. Once the VM completely stops. Click Actions Start.

[start=18]
. See where the pod-affinity-vm and node-affinity-vm are running by looking at OpenShift Console or check on the OpenShift Command line. 

[source,bash]
----
oc get vmi pod-affinity-vm
oc get vmi node-affinity-vm
----

[start=19]
. You will now see the the pod-affinity-vm is now running the same location as the node-affinity-vm because of the pod affinity rule.


[#podantiaffinity]
== Pod Anti-Affinity

Pod Anti-affinity is a crucial feature for achieving High Availability (HA) in virtualized environments. It functions by instructing the scheduler to prevent the co-location of VM Pods on the same node if they possess a specific label.

The primary benefit of this rule is to enhance application resilience. For instance, by ensuring that VM Pods belonging to the same service (such as a database cluster) are distributed across different nodes, a failure in a single node will not cause an outage for the entire service.

This lab will guide you through setting up and demonstrating the functionality of Pod Anti-Affinity.


[start=1]
. We are going to use the node-affinity-vm and reuse the app: node-affinity-vm label we created last time to create our pod anti affinity rule.
[start=2]
. Click the pod-affinity-vm  

[start=3]
. Click Configure → scheduling

[start=4]
. Click the Affinity rules

[start=5]
. Add Affinity rule

image::exercise4/04-image-affinity06.png[title="Add Pod Anti-Affinity Rule", link=self, window=blank, width=100%]

[start=6]
. Change the type to  Workload (pod) Anit-Affinity

[start=7]
. Keep the condition to Required during scheduling.

[start=8]
. Keep the Topology key they same 

[start=9]
. Click Add expression under Workload labels

[start=10]
. Next, we will set a key and value to establish Pod Anti-affinity
, ensuring this VM runs on the same node as node-affinity-vm. Set the key to app and the value to node-affinity-vm. The resulting Pod Affinity rule should match the illustration provided below. Verify the rule's correctness, then click Save affinity rule.

image::exercise4/04-image-affinity06.png[title="Add Pod Anti-Affinity Rule", link=self, window=blank, width=100%]

[start=11]
. Click Apply Rules

[start=12]
. Now let's take a look at the Pod Anto Affinity rule on pod-anti-affinity-vm. You can do this by using the GUI and going to YAML on the pod-anti-affinity-vm or you can do it on the CLI.

[source,bash]
----
oc edit vm pod-anti-affinity-vm 
----

[source,bash]
----
:q!
----


[start=13]
. See where the pod-affinity-vm and node-affinity-vm are running by looking at OpenShift Console or check on the OpenShift Command line. 

[source,bash]
----
oc get vmi pod-anti-affinity-vm
oc get vmi node-affinity-vm
----

[start=14]
. To make the vm follow these new affinity rules. You normally would migrate it to the node with the label. However, to show that this affinity rule is working. We are going to power off the vm completely and then power it back on.

[start=15]
. On the pod-anti-affinity-vm Click Action stop.

[start=16]
. Once the VM completely stops. Click Actions Start.

[start=17]
. See where the pod-affinity-vm and node-affinity-vm are running by looking at OpenShift Console or check on the OpenShift Command line. 

[source,bash]
----
oc get vmi pod-anti-affinity-vm
oc get vmi node-affinity-vm
----

[start=18]
. The  pod-anit-affinity-vm will now be running on a different node than node-affinity-vm. If pod-anit-affinity-vm was not already running on a different node then the node-affinity-vm. 
