= Affinity and Anti-Affinity for VM Placement

This lab is designed to enhance your proficiency in applying Node Affinity, Pod Affinity, and Anti-Pod Affinity specifically to Virtual Machines. The core purpose is to illustrate the practical application of these principles in real-world scenarios and solidify your conceptual understanding.

include::lab-access.adoc[]

[#nodeaffinity]
== Node Affinity

Node Affinity defines rules that guide the scheduler to attract a Virtual Machine Pod to a specific node or group of nodes. These rules rely on matching labels applied to the nodes.

The core use case for Node Affinity is to ensure that a VM runs only on nodes that possess specific hardware features, such as a particular GPU model or a high amount of RAM, by matching the corresponding labels on the node.

This lab will demonstrate how Node Affinity is set up and how it functions.

[#nodeaffinityinstructions]
=== Instructions

[start=1]
. Ensure you are <<labaccess,logged in to both the OpenShift Console and CLI as the *admin* user>> from your *web browser* and the *terminal* window on the right side of your screen and continue to the next step.

+
. See where the the node-affinity-vm is running by looking at OpenShift Console and navigating to *Virtualization →  Projects → affinity* or check on the OpenShift Command line.

+
[source,text,role=execute]
.Change to affinity project
----
oc project affinity
----

+
[.wrap,console,subs="attributes"]
----
Now using project "affinity" on server {openshift_api_server_url}.
----

+
[source,text,role=execute]
.Get VM information
----
oc get vmi node-affinity-vm
----

+
----
NAME               AGE   PHASE     IP            NODENAME                 READY
node-affinity-vm   78m   Running   10.235.0.25   worker-cluster-t96sv-1   True
----

+
. Set an east node label on any work where the node-affinity-vm is currently not running.

+
[source,text,role=execute]
.Get node list
----
oc get nodes
----

+
.Output
----
NAME                            STATUS   ROLES                         AGE     VERSION
control-plane-cluster-t96sv-1   Ready    control-plane,master,worker   4h38m   v1.33.5
control-plane-cluster-t96sv-2   Ready    control-plane,master,worker   4h23m   v1.33.5
control-plane-cluster-t96sv-3   Ready    control-plane,master,worker   4h37m   v1.33.5
worker-cluster-t96sv-1          Ready    worker                        88m     v1.33.5
worker-cluster-t96sv-2          Ready    worker                        87m     v1.33.5
worker-cluster-t96sv-3          Ready    worker                        86m     v1.33.5
----

+
[source,text,role=execute]
.Label node
----
oc label node <worker where node-affinity-vm is not running> zone=east
----

+
.Output
----
node/worker-cluster-t96sv-3 labeled
----

+
Alternatively you can use the OpenShift Console by going to *Compute → Nodes* and then pick a node where *node-affinity-vm* is not running. Then click the 3 dots and click *Edit labels*. Once finished adding the label click *Save*.

+
image::exercise4/04-image-affinity.png[title="Label Nodes", link=self, window=blank, width=100%]

+
. Make sure the label was applied.

+
[source,text,role=execute]
.Find node with the assigned label
----
oc get nodes <worker you just labeled> --show-labels | grep -i zone=east
----

+
Alternatively you can use the OpenShift Console by going to *Compute → Nodes* and then pick a node you just label. Then click the 3 dots and click *Edit labels*. Once finished looking click *Save*.

+
. Navigate to *Virtualization → VirtualMachines → node-affinity-vm → Configuration* 

+
image::exercise4/04-image-affinity01.png[title="Node Affinity Navigation", link=self, window=blank, width=100%]

+
. Now navigate to *Scheduling → Affinity* rules

+
image::exercise4/04-image-affinity02.png[title="Node Affinity add rule", link=self, window=blank, width=100%]

+
. Click *Add affinity rule*.

+
image::exercise4/04-image-affinity02a.png[title="Node Affinity add rule", link=self, window=blank, width=100%]

+
. Change the *Condition* to *Preferred during scheduling* and set the *weight* to *75*.
Then, under *Node Labels* click *Add Expression*. Set the *Key* field to `zone` and the *Values* field to `east` and click *add*. These are the same label applied to the node earler. Your Final Node Affinity rule will look like the picture below. Confirm this is true and click *Save affinity rule*.

+
image::exercise4/04-image-affinity03.png[title="Node Affinity Rule", link=self, window=blank, width=100%]

+
. Then click *Apply rules*.

+
image::exercise4/04-image-affinity03a.png[title="Node Affinity Rule", link=self, window=blank, width=100%]

+
. Now let's take a look at the Node Affinity rule on *node-affinity-vm*. You can view this information either through the GUI by navigating to *node-affinity-vm → YAML*, or by using the CLI.

+
[source,text,role=execute]
.View the VM definition
----
oc get vm node-affinity-vm -n affinity -o jsonpath='{.spec.template.spec.affinity}'
----

+
----
{"nodeAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"preference":{"matchExpressions":[{"key":"zone","operator":"In","values":["east"]}]},"weight":75}]}}
----

+
. To make the VM follow these new affinity rules, you could migrate it to the node with the label. However, to show that this affinity rule is working we are going to power off the VM completely and then power it back on. 

+
. Click *Actions → Stop*.

+
. Once the VM completely stops. Click *Actions → Start*.

+
. The VM's relocation to the node with the assigned label is visible via the GUI. Alternatively, you can verify this change using the OCP CLI.

+
[source,text,role=execute]
.Get VMI information
----
oc get vmi node-affinity-vm
---- 

+
----
NAME               AGE   PHASE     IP            NODENAME                 READY
node-affinity-vm   58s   Running   10.233.2.20   worker-cluster-t96sv-3   True
----

[#podaffinity]
== Pod Affinity
Pod Affinity is a scheduling rule that co-locates a VM Pod and an existing Pod (or another VM Pod) with a specific label onto the same node.

The primary benefit of using pod affinity is to improve performance for dependent VMs or services that require low-latency communication by guaranteeing their placement on the same worker node.

This lab will illustrate the setup and function of Pod Affinity.

[#podaffinityinstructions]
=== Instructions

[start=1]
. Ensure you are <<labaccess,logged in to both the OpenShift Console and CLI as the *admin* user>> from your *web browser* and the *terminal* window on the right side of your screen and continue to the next step.

+
. To begin, we need to apply a label to the *node-affinity-vm*. This will enable it to serve as the VM that our pod will have pod affinity to. The label can be added by editing the VM within the OpenShift CLI and including app: fedora

+
[source,text,role=execute]
.Modify the VM
----
oc edit vm node-affinity-vm -n affinity
----
+
[source,yaml,role=execute]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  annotations:
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    argocd.argoproj.io/sync-wave: "10"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"kubevirt.io/v1","kind":"VirtualMachine","metadata":{"annotations":{"argocd.argoproj.io/sync-options":"SkipDryRunOnMissingResource=true","argocd.argoproj.io/sync-wave":"10"},"labels":{"app.kubernetes.io/instance":"module-affinity"},"name":"node-affinity-vm","namespace":"affinity"},"spec":{"dataVolumeTemplates":[{"metadata":{"name":"node-affinity-vm-volume"},"spec":{"sourceRef":{"kind":"DataSource","name":"rhel10","namespace":"openshift-virtualization-os-images"},"storage":{"resources":{"requests":{"storage":"30Gi"}}}}}],"instancetype":{"name":"u1.small"},"preference":{"name":"rhel.10"},"runStrategy":"Manual","template":{"metadata":{"labels":{"network.kubevirt.io/headlessService":"headless"}},"spec":{"domain":{"devices":{"autoattachPodInterface":false,"disks":[{"disk":{"bus":"virtio"},"name":"cloudinitdisk"}],"interfaces":[{"masquerade":{},"name":"default"}]}},"networks":[{"name":"default","pod":{}}],"subdomain":"headless","volumes":[{"dataVolume":{"name":"node-affinity-vm-volume"},"name":"rootdisk"},{"cloudInitNoCloud":{"userData":"#cloud-config\nchpasswd:\n  expire: false\npassword: redhat\nuser: rhel\n"},"name":"cloudinitdisk"}]}}}}
    kubemacpool.io/transaction-timestamp: "2025-12-13T00:21:52.599027397Z"
    kubevirt.io/latest-observed-api-version: v1
    kubevirt.io/storage-observed-api-version: v1
  creationTimestamp: "2025-12-12T20:44:59Z"
  finalizers:
  - kubevirt.io/virtualMachineControllerFinalize
  generation: 2
  labels:
    app.kubernetes.io/instance: module-affinity  <--- Do not put the label here
  name: node-affinity-vm
  namespace: affinity
  resourceVersion: "328565"
  uid: 6e05ce41-019f-498a-a07f-ef4e9c93ddd3
spec:
  dataVolumeTemplates:
  - metadata:
      creationTimestamp: null
      name: node-affinity-vm-volume
    spec:
      sourceRef:
        kind: DataSource
        name: rhel10
        namespace: openshift-virtualization-os-images
      storage:
        resources:
          requests:
            storage: 30Gi
  instancetype:
    kind: virtualmachineclusterinstancetype
    name: u1.small
  preference:
    kind: virtualmachineclusterpreference
    name: rhel.10
  runStrategy: Manual
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: fedora   <--- Put the label here
        network.kubevirt.io/headlessService: headless
----
+
[source,text,role=execute]
----
:wq!
----

+
. To make the label get applied to the vmi of the node-affinity-vm. We will reboot it.

+
. Using the pod-affinity-vm VM, Click on *Action → Stop*.

+
. Once the VM completely stops. Click *Actions → Start*.

+
. Click the pod-affinity-vm  

+
. See where the pod-affinity-vm is running by looking at OpenShift Console or check on the OpenShift Command line. 

+
[source,text,role=execute]
.Get VMI information
----
oc get vmi pod-affinity-vm
----

+
. Click *Configure → Scheduling*.

+
. Click the *Affinity rules*.

+
image::exercise4/04-image-affinity04.png[title="Pod Affinity Rule", link=self, window=blank, width=100%]

+
. Click *Add affinity rule*.

+
. Change the type to *Workload (pod) Affinity*

+
. Keep the *Condition* set to *Required during scheduling*.

+
. Leave the *Topology key* at the default value.

+
. Click *Add expression* under *Workload labels*.

+
. You will now set a key so this VM will run on the same node as *node-affinity-vm*. Set the *Key* field to `app` and the *Values* field to `fedora` and click *add*. Your Final Pod Affinity rule will look like the picture below. Confirm this is true and click *Save affinity rule*.

+
image::exercise4/04-image-affinity05.png[title="Pod Affinity Rule", link=self, window=blank, width=100%]

+
. Click *Apply rules*

+
. Now let's take a look at the Pod Affinity rule on *pod-affinity-vm*. You can view this information either through the GUI by navigating to *pod-affinity-vm → YAML*, or by using the CLI.

+
[source,text,role=execute]
----
oc get vm pod-affinity-vm -n affinity -o jsonpath='{.spec.template.spec.affinity}'
---- 

+
[source,text,role=execute]
----
{"podAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["fedora"]}]},"topologyKey":"kubernetes.io/hostname"}]}}
----

+
. See where the pod-affinity-vm and node-affinity-vm are running by looking at OpenShift Console or check on the OpenShift Command line. 

+
[source,text,role=execute]
----
oc get vmi pod-affinity-vm
oc get vmi node-affinity-vm
----

+
. To make the vm follow these new affinity rules. You normally would migrate it to the node with the label. However, to show that this affinity rule is working. We are going to power off the vm completely and then power it back on. 

+
. Using the *pod-affinity-vm* VM, Click on *Action → Stop*.

+
. Once the VM completely stops. Click *Actions → Start*.

+
. See where the *pod-affinity-vm* and *node-affinity-vm* are running by looking at OpenShift Console or check on the OpenShift Command line. 

+
[source,text,role=execute]
----
oc get vmi pod-affinity-vm
oc get vmi node-affinity-vm
----

+
. You will now see the the *pod-affinity-vm* is now running the same location as the *node-affinity-vm* because of the pod affinity rule.


[#podantiaffinity]
== Pod Anti-Affinity

Pod Anti-affinity is a crucial feature for achieving High Availability (HA) in virtualized environments. It functions by instructing the scheduler to prevent the co-location of VM Pods on the same node if they possess a specific label.

The primary benefit of this rule is to enhance application resilience. For instance, by ensuring that VM Pods belonging to the same service (such as a database cluster) are distributed across different nodes, a failure in a single node will not cause an outage for the entire service.

This lab will guide you through setting up and demonstrating the functionality of Pod Anti-Affinity.

[#podantiaffinityinstructions]
=== Instructions

[start=1]
. Ensure you are <<labaccess,logged in to both the OpenShift Console and CLI as the *admin* user>> from your *web browser* and the *terminal* window on the right side of your screen and continue to the next step.

+
. We are going to use the *pod-anti-affinity-vm* and reuse the *app: fedora* label we created last time to create our pod anti affinity rule.

+
. Click the *pod-anti-affinity-vm*  

+
. Click *Configure → scheduling*

+
. Click the *Affinity rules*

+
. *Add Affinity rule*

+
image::exercise4/04-image-affinity06.png[title="Add Pod Anti-Affinity Rule", link=self, window=blank, width=100%]

+
. Change the type to *Workload (pod) Anti-Affinity*

+
. Keep the *Condition* set to *Required during scheduling*.

+
. Keep the *Topology key* the same 

+
. Click *Add expression* under *Workload labels*

+
. Next, we will set a *key* and *value* to establish Pod Anti-affinity, ensuring this VM runs on a different node than *pod-affinity-vm*. Set the *key* to `app` and the *value* to `fedora` and click *add*. The resulting Pod Affinity rule should match the illustration provided below. Verify the rule's correctness, then click Save affinity rule.

+
image::exercise4/04-image-affinity07.png[title="Add Pod Anti-Affinity Rule", link=self, window=blank, width=100%]

+
. Click *Apply Rules*

+
. Now let's take a look at the Pod Anti-Affinity rule on *pod-anti-affinity-vm*. You can view this information either through the GUI by navigating to pod-anti-affinity-vm → YAML, or by using the CLI.

+
[source,text,role=execute]
----
oc get vm pod-anti-affinity-vm -n affinity -o jsonpath='{.spec.template.spec.affinity}'
----
+
[source,text,role=execute]
----
{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["fedora"]}]},"topologyKey":"kubernetes.io/hostname"}]}}
----

+
. See where the pod-affinity-vm and node-affinity-vm are running by looking at OpenShift Console or check on the OpenShift Command line. 

+
[source,text,role=execute]
----
oc get vmi pod-anti-affinity-vm
oc get vmi node-affinity-vm
----

+
. To make the VM follow these new affinity rules. You normally would migrate it to the node with the label. However, to show that this affinity rule is working. We are going to power off the vm completely and then power it back on.

+
. On the *pod-anti-affinity-vm* virtual machine, Click *Action → Stop*.

+
. Once the VM completely stops. Click *Actions → Start*.

+
. See where the *pod-affinity-vm* and *pod-anti-affinity-vm* are running by looking at OpenShift Console or check on the OpenShift Command line. 

+
[source,text,role=execute]
----
oc get vmi pod-anti-affinity-vm
oc get vmi node-affinity-vm
----

+
. The *pod-anti-affinity-vm* VM will now be running on a different node than *node-affinity-vm*. If *pod-anti-affinity-vm* was not already running on a different node than the *node-affinity-vm*. 
