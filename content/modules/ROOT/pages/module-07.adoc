= Decentralized Live Migration

A decentralized live migration is a variation of a storage live migration that allows you to migrate a running `VirtualMachine` between namespaces and clusters.

This can be useful to:

*Balance Workloads*: Distributing virtual machines between clusters helps optimize resource utilization. If one cluster is heavily loaded while another is idle, rebalancing can significantly improve operational efficiency.

*Facilitate Maintenance*: For environments with multiple clusters, virtual machine migration allows for seamless maintenance. You can move virtual machines off a cluster slated for upgrades or shutdown, ensuring zero downtime for your services.

*Expedite Restores*: Instant restore capabilities from backup vendors, particularly when coupled with namespace migration, can drastically speed up recovery times. virtual machines can be quickly restored to a temporary location and then migrated to their original namespace and storage.

This lab demonstrates the forthcoming Tech Preview capabilities of Decentralized Live Migration. In this lab, we will walk through the live migration of a `VirtualMachine` from one `Namespace` to another. The process of live migrating between namespaces follows the same process as if you were to migrate a `VirtualMachine` between 2 clusters.


[NOTE]
=====
The UI based workflow is not fully functional yet. During this lab we will introduce you to the UI workflow, but perform the migration through the CLI.
=====

[#howdeclm]
== How does it work?

The migration involves two `VirtualMachineInstances` and two `VirtualMachineInstanceMigration` objects. Like a storage live migration, disk contents are copied over the network to the receiving `VirtualMachine`. The key difference is that the receiving virtual machine has a completely separate `VirtualMachineInstance`. In order to coordinate the migration, the status of the source and target `VirtualMachineInstance` has to be synchronized. A dedicated synchronization controller, running in the `openshift-cnv` namespace facilitates communication between the source and target `VirtualMachineInstances`.

[#decmlrequirements]
== Requirements

These requirements have already been enabled on your cluster and are noted for your reference.

* The `Migration Toolkit for Virtualization (MTV)` Operator must be installed with the feature `feature_ocp_live_migration` set to `true` when creating the `ForkliftController` CR
* You must enable the `DecentralizedLiveMigration` `featureGate` on the `KubeVirt` CR

[#decmlinstructions]
== Instructions

. Using the credentials provided by the lab administrator, log in to the OpenShift CLI from your terminal on the bastion host. Be sure to substitute the placeholder password and cluster name with the specific values you were given.

. Verify the source `VirtualMachineInstance` is running.
+
[.wrap,console,role=execute]
----
oc get vmi -n vm-live-migration-source
----
+
The output will look similar to the following, with a different `IP` and `NODENAME`:

+
[source,console]
----
NAME                     AGE  PHASE    IP          NODENAME               READY
vm-migration-ns-ns-live  28m  Running  10.233.2.16 worker-cluster-hflz6-3 True
----

+
Now let's migrate our `VirtualMachine` `vm-migration-ns-ns-live` from the `Namespace` `vm-live-migration-source` to a new `Namespace` `vm-live-migration-destination`.

[start=3]
. As we noted above, the migration requires two `VirtualMachineInstances`.

+ 
The first step is to create an empty `DataVolume` for the receiver `VirtualMachine` in the destination `Namespace`. +

+ 
Execute the following command to create the destination `DataVolume`:

+ 
[.wrap,yaml,role=execute]
----
cat <<EOF | oc apply -f -
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  annotations:
    cdi.kubevirt.io/storage.usePopulator: "true"
  name: vm-migration-ns-ns-live
  namespace: vm-live-migration-destination
spec:
  source:
    blank: {}
  storage:
    storageClassName: ocs-external-storagecluster-ceph-rbd
    resources:
      requests:
        storage: 30Gi
EOF
----

+ 
Confirm the `DataVolume` and associated `PersistentVolumeClaim` have been created by executing the following 2 commands:

+ 
[.wrap,console,role=execute]
----
oc get DataVolume -n vm-live-migration-destination
----

+ 
[source,console]
----
NAME                     PHASE              PROGRESS  RESTARTS  AGE
vm-migration-ns-ns-live  PendingPopulation  N/A                 107s
----

+ 
[.wrap,console,role=execute]
----
oc get PersistentVolumeClaim -n vm-live-migration-destination
----

+ 
[source,console]
----
NAME                     STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS                          VOLUMEATTRIBUTESCLASS  AGE
vm-migration-ns-ns-live  Pending                                  ocs-external-storagecluster-ceph-rbd  <unset>                2m5s
----
+ 

[start=4]
. Next, create the receiver `VirtualMachine` in the destination `Namespace`

+ 
The destination `VirtualMachine` is same as the source `VirtualMachine` except for 2 key differences: 

+
[loweralpha]
.. The destination `VirtualMachine` has an `annotation` to set the post live migration `runStrategy`

+ 
[source,yaml]
----
    kubevirt.io/restore-run-strategy: Always
----

+ 
[loweralpha,start=2]
.. The destination `VirtualMachine` has different `spec.runStrategy` 

+ 
[source,yaml]
----
  runStrategy: WaitAsReceiver
----

+ 
. Execute the following command to create the destination `VirtualMachine`:

+
[.wrap,yaml,role=execute]
----
cat <<EOF | oc apply -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  annotations:
    kubevirt.io/restore-run-strategy: Always
  name: vm-migration-ns-ns-live
  namespace: vm-live-migration-destination
spec:
  runStrategy: WaitAsReceiver
  template:
    metadata:
      annotations:
        vm.kubevirt.io/flavor: small
        vm.kubevirt.io/os: rhel9
        vm.kubevirt.io/workload: server
      creationTimestamp: null
      labels:
        kubevirt.io/domain: vm-migration-ns-ns-live
        kubevirt.io/size: small
        network.kubevirt.io/headlessService: headless
    spec:
      architecture: amd64
      networks:
      - name: default
        pod: {}
      domain:
        cpu:
          cores: 1
          sockets: 1
          threads: 1
        devices:
          disks:
          - disk:
              bus: virtio
            name: rootdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
          interfaces:
          - macAddress: 02:a1:3b:00:00:85
            masquerade: {}
            model: virtio
            name: default
          logSerialConsole: false
          rng: {}
        features:
          acpi: {}
          smm:
            enabled: true
        firmware:
          bootloader:
            efi: {}
          serial: 94f58dd1-b1e1-4aab-add2-ab3d3483d297
        machine:
          type: pc-q35-rhel9.6.0
        memory:
          guest: 2Gi
        resources: {}
      terminationGracePeriodSeconds: 180
      volumes:
      - dataVolume:
          name: vm-migration-ns-ns-live
        name: rootdisk
      - cloudInitNoCloud:
          userData: |-
            #cloud-config
            user: cloud-user
            password: redhat
            chpasswd: { expire: False }
        name: cloudinitdisk
EOF
----

+
Confirm the `VirtualMachine` and associated `VirtualMachineInstance` have been created by executing the following 2 commands:

+
[.wrap,console,role=execute]
----
oc get VirtualMachine
----

+
[source,console]
----
NAME                      AGE     STATUS               READY
vm-migration-ns-ns-live   5m51s   WaitingForReceiver   False
----

+
[.wrap,console,role=execute]
----
oc get VirtualMachineInstance
----

+
[source,console]
----
NAME                      AGE     PHASE            IP    NODENAME   READY
vm-migration-ns-ns-live   5m55s   WaitingForSync                    False
----

+
You will notice that the `VirtualMachine` and `VirtualMachineInstance` have a unique `PHASE`, `WaitingForReceiver` and `WaitingForSync`. This indicates the `VirtualMachine` is waiting for the migration to start as a receiver and the `VirtualMachineInstance` is waiting for data to be synchronized from the source `VirtualMachine`.

+
[start=6]
. With the destination `VirtualMachine` and `VirtualMachineInstance` waiting, create the destination `VirtualMachineInstanceMigration` by executing the following command:

+ 
[.wrap,yaml,role=execute]
----
cat <<EOF | oc apply -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstanceMigration
metadata:
  name: ns-to-ns-vm-live-migration-instance-destination
  namespace: vm-live-migration-destination
spec:
  receive:
    migrationID: 52e4398d-bdbf-42b5-b0f4-1e7c6c0a08f5-38cec1f6-43bb-412d-8477-b3d635fd7123
  vmiName: vm-migration-ns-ns-live
EOF
----

+
Confirm the destination `VirtualMachineInstanceMigration` has been created by executing the following command:

+ 
[.wrap,console,role=execute]
----
oc get VirtualMachineInstanceMigration
----

+ 
[source,console]
----
NAME                                              PHASE            VMI
ns-to-ns-vm-live-migration-instance-destination   WaitingForSync   vm-migration-ns-ns-live
----

[start=7]
. With the destination `VirtualMachineInstanceMigration` waiting, the final step is to create the source `VirtualMachineInstanceMigration`.

+
To create the source `VirtualMachineInstanceMigration`, we need the `IP address` of the leader `virt-synchronization-controller`.

+
[loweralpha]
.. Find the leader `virt-synchronization-controller` by looking at the lease holder:

+
[.wrap,console,role=execute]
----
oc get leases -n openshift-cnv | grep virt-synchronization-controller
----

+
[source,console]
----
virt-synchronization-controller     virt-synchronization-controller-65c7b9d5bd-f4rbg        147m
----

+
[loweralpha,start=2]
.. Find the `IP address` of the leader `virt-synchronization-controller` using the `Pod` name from the previous command:

+
[.wrap,console,role=execute]
----
oc get pods -o wide -n openshift-cnv | grep 'virt-synchronization-controller-65c7b9d5bd-f4rbg'
----

+
[source,console]
----
virt-synchronization-controller-65c7b9d5bd-f4rbg     1/1   Running  0    150m   10.233.0.222   control-plane-cluster-hflz6-1  <none>   <none>
----

+
. Take the `Pod` `IP address` from the previous command output (`10.233.0.222` in the example) and replace `<leader_sync_controller_ip>` in the `VirtualMachineInstanceMigration` YAML below. Take care that you do not remove the port `:9185`

+
[.wrap,yaml,role=execute]
----
cat <<EOF | oc apply -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstanceMigration
metadata:
  name: ns-to-ns-vm-live-migration-instance-source
  namespace: vm-live-migration-source
spec:
  sendTo:
    connectURL: <leader_sync_controller_ip>:9185
    migrationID: 52e4398d-bdbf-42b5-b0f4-1e7c6c0a08f5-38cec1f6-43bb-412d-8477-b3d635fd7123
  vmiName: vm-migration-ns-ns-live
EOF
----

[#decmlmonitor]
== Monitor the Migration
When the source `VirtualMachineInstanceMigration` above is created, the `Migration` will start. You can monitor the `Migration` through a few different lenses:

. *Using the CLI*

+
[.wrap,console,role=execute]
----
oc get vmim -A -w
----

+
[NOTE]
=====
Using `-w` to apply a watch, you will see the `Migration` progress from `Scheduling` through to `Succeeded`.
=====

+
[source,console]
----
NAMESPACE                       NAME                                              PHASE        VMI
vm-live-migration-destination   ns-to-ns-vm-live-migration-instance-destination   Scheduling   vm-migration-ns-ns-live
vm-live-migration-source        ns-to-ns-vm-live-migration-instance-source        Scheduling   vm-migration-ns-ns-live
vm-live-migration-destination   ns-to-ns-vm-live-migration-instance-destination   Scheduled    vm-migration-ns-ns-live
vm-live-migration-destination   ns-to-ns-vm-live-migration-instance-destination   PreparingTarget   vm-migration-ns-ns-live
vm-live-migration-source        ns-to-ns-vm-live-migration-instance-source        Scheduled         vm-migration-ns-ns-live
vm-live-migration-source        ns-to-ns-vm-live-migration-instance-source        PreparingTarget   vm-migration-ns-ns-live
vm-live-migration-destination   ns-to-ns-vm-live-migration-instance-destination   TargetReady       vm-migration-ns-ns-live
vm-live-migration-source        ns-to-ns-vm-live-migration-instance-source        TargetReady       vm-migration-ns-ns-live
vm-live-migration-source        ns-to-ns-vm-live-migration-instance-source        Running           vm-migration-ns-ns-live
vm-live-migration-destination   ns-to-ns-vm-live-migration-instance-destination   Running           vm-migration-ns-ns-live
vm-live-migration-destination   ns-to-ns-vm-live-migration-instance-destination   Succeeded         vm-migration-ns-ns-live
----

+
[.wrap,console,role=execute]
----
oc get vm
----

+
The `VirtualMachine` will show you it is `Migrating`.

+
[source,console]
----
NAME                      AGE    STATUS      READY
vm-migration-ns-ns-live   7m1s   Migrating   False
----

+
[start=2]
. *Using the Console*

+
Login to the OpenShift console using the credentials provided to you by the lab administrator.

+
From the left hand menu, navigate to `Virtualization` -> `Virtual Machines`, click the `Namespace` `vm-live-migration-source` to expand it and click on the virtual machine named `vm-migration-ns-ns-live`.

+
image::exercise7/image1-ui-src-vm-running.png[title="Source Virtual Machine Details - Running", link=self, window=blank, width=100%]

+

image::exercise7/image2-ui-dest-waiting.png[title="Destination Virtual Machine Details - WaitingForReceiver", link=self, window=blank, width=100%]

+
image::exercise7/image3-ui-dest-starting.png[title="Destination Virtual Machine Details - Running", link=self, window=blank, width=100%]

+
image::exercise7/image4-ui-dest-migrating.png[title="Destination Virtual Machine Details - Migrating", link=self, window=blank, width=100%]

+
image::exercise7/image5-ui-migrating-overview-panel.png[title="Destination Virtual Machine - Migration Pop-up Panel", link=self, window=blank, width=100%]

+
image::exercise7/image6-ui-dest-migrating-metrics.png[title="Destination Virtual Machine - Migration Metrics", link=self, window=blank, width=100%]

+
image::exercise7/image7-ui-dest-migrating-metrics2.png[title="Destination Virtual Machine - Migration Metrics", link=self, window=blank, width=100%]

+
image::exercise7/image8-ui-dest-migrating-progress-bar.png[title="Destination Virtual Machine - Migration Progress Bar", link=self, window=blank, width=100%]

+
image::exercise7/image9-ui-dest-running-src-stopped.png[title="Destination Virtual Machine - Running & Source Virtual Machine - Stopped, link=self, window=blank, width=100%]

+
