= Overcommit in OpenShift Virtualization

== Introduction


*Overcommit* occurs when the total virtual resources allocated to VMs exceed the physical resources available on the host, enabling higher workload density by leveraging the fact that VMs rarely use their full allocated capacity simultaneously.

[#cpu]
== CPU Overcommit

In OpenShift Virtualization, compute resources assigned to virtual machines (VMs) are backed by either *guaranteed CPUs* or *time-sliced CPU shares*.

*Guaranteed CPUs*, also known as CPU reservation, dedicate CPU cores or threads to a specific workload, making them unavailable to any other workload. Assigning guaranteed CPUs to a VM ensures sole access to a reserved physical CPU. You enable dedicated resources for VMs to use guaranteed CPUs.

*Time-sliced CPUs* dedicate a slice of time on a shared physical CPU to each workload. You can specify the slice size during VM creation or when the VM is offline. By default, each vCPU receives 100 milliseconds (1/10 of a second) of physical CPU time.

With time-sliced CPUs, the Linux kernel's *Completely Fair Scheduler (CFS)* manages how VMs share physical CPU cores. CFS rotates VMs through available cores, giving each VM a proportional slice of CPU time based on its configured CPU requests.


To allow for CPU overcommit, which is set to 10:1 by default, each VM’s virt-launcher pod will define “100m” total CPU requests, which is 1/10th of a host CPU from a Kubernetes resource and scheduling perspective, per vCPU.

However this default 10:1 CPU overcommit ratio can be configured to the desired overcommit level by changing the cluster CPU Allocation Ratio. Changing this ratio will influence the amount of cpu requests each vCPU is allocated by default, which enforces a max level of CPU overcommit through Kubernetes requests scheduling.

NOTE: Note that resource assignments are made at virt-launcher pod scheduling time, so any VMs will need to be live migrated stopped and restarted to change CPU allocation behavior after a ratio change.

=== Instructions

[start=1]
. Login into your bastion host with credentials provided to you by the lab administrator. You will need this for later steps.

[source,bash]
----	
ssh lab-user@<host> -p 32270
----

[start=2]
. Using the credentials provided by the lab administrator, log in to the OpenShift CLI from your terminal on the bastion host. Be sure to substitute the placeholder password and cluster name with the specific values you were given.

[source,bash]
----
oc login -u admin -p <password> --server=https://api.cluster-<clustername>.dynamic.redhatworkshops.io:6443
oc login whoami --show-console
----

[start=3]
. To understand CPU overcommit, you must first identify how many physical CPU cores are available on your worker nodes.

* List all worker nodes in your cluster:


**Using the CLI:**

[source,bash]
----
oc get nodes -l node-role.kubernetes.io/worker=
NAME                     STATUS   ROLES    AGE     VERSION
worker-cluster-pg8lt-1   Ready    worker   4h59m   v1.33.5
worker-cluster-pg8lt-2   Ready    worker   4h58m   v1.33.5
worker-cluster-pg8lt-3   Ready    worker   4h58m   v1.33.5
----


[source,bash]
----
oc describe node <node_name> | grep -A 5 "Capacity:"
----

This command displays output similar to:

[source,text]
----
Capacity:
  cpu:                            16
  devices.kubevirt.io/kvm:        1k
  devices.kubevirt.io/tun:        1k
  devices.kubevirt.io/vhost-net:  1k
  ephemeral-storage:              104266732Ki
  hugepages-1Gi:                  0
  hugepages-2Mi:                  0
  memory:                         65836964Ki
  pods:                           250
----

The `cpu` value shows the number of physical CPU cores (or threads if hyperthreading is enabled) available on the node.


* Alternatively, check CPU allocatable resources (physical CPUs minus system reservations):

[source,bash]
----
oc get node <node_name> -o jsonpath='{.status.allocatable.cpu}{"\n"}'
15500m
----

**Using the OpenShift Console:**
Navigate to Compute → Nodes

image::exercise2/node-details.png[title="Confirm number of CPUs", link=self, window=blank, width=100%]

[start=4]
. Viewing VM vCPU Allocations

**Using the CLI:**
List all running VMs and their vCPU counts:

[source,bash]
----
oc get vms -n over-commit -o custom-columns=\
NAMESPACE:.metadata.namespace,\
NAME:.metadata.name,\
vCPUs:.spec.template.spec.domain.cpu.cores,\
STATUS:.status.printableStatus
NAMESPACE     NAME              vCPUs   STATUS
over-commit   overcommit-vm-1   24      Running
----

For a specific VM, check the vCPU configuration:

[source,bash]
----
oc get vm overcommit-vm-1 -n over-commit -o jsonpath='{.spec.template.spec.domain.cpu.cores}{"\n"}'
24
----

View CPU requests and limits for a running VM:

[source,bash]
----
oc get vmi overcommit-vm-1 -n over-commit -o jsonpath='{.spec.domain.cpu}{"\n"}'
{"cores":24,"maxSockets":8,"model":"host-model","sockets":2,"threads":1}
----

**Using the OpenShift Console:**
Navigate to Virtualization → Virtual Machines and select the overcommit-vm-1 VM.

image::exercise2/vm-details.png[title="Confirm number of CPUs", link=self, window=blank, width=100%]

[start=5]
. Confirm the number of CPUs inside the guest

From the web console, login into the VM and run the following command to verify the number of CPUs:
[source,bash]
----
nproc
48
----

image::exercise2/overcommit-nproc.png[title="Confirm number of CPUs", link=self, window=blank, width=100%]



[start=6]
. Calculating CPU Overcommit Ratio

To calculate the overcommit ratio on a specific node:

* Identify all VMs running on the node:

[source,bash]
----
oc get vmi -A -o wide | grep <node_name>
affinity           pod-anti-affinity-vm    4h19m   Running   10.133.2.11   worker-cluster-pg8lt-3   True    True
over-commit        overcommit-vm-1         47m     Running   10.133.2.25   worker-cluster-pg8lt-3   True    True
----

* Sum the vCPUs for all VMs on that node:

[source,bash]
----
oc get vmi -A -o json | \
jq -r --arg NODE "<node_name>" \
'.items[] | select(.status.nodeName == $NODE) | 
{
  name: .metadata.name,
  total_vcpus: ((.spec.domain.cpu.sockets // 1) * (.spec.domain.cpu.cores // 1) * (.spec.domain.cpu.threads // 1))
}' | \
jq -s 'map(.total_vcpus) | add'
48
----

* Calculate the overcommit ratio:

[source,text]
----
Overcommit Ratio = Total vCPUs allocated / Physical CPU cores

Example:
- Physical CPU cores: 16
- Total vCPUs allocated: 48
- Overcommit ratio: 48 / 16 = 3:1
----


[start=7]
. Understanding the Default 10:1 CPU Overcommit Ratio

OpenShift Virtualization applies a default *10:1 CPU overcommit ratio* when you don't explicitly specify CPU requests. This means that if a VM has multiple vCPUs, the actual CPU request on the virt-launcher pod is only 1/10th of the total vCPUs.

* Check virt-launcher Pod CPU Requests

To see the actual CPU requests made by the virt-launcher pod (which runs the VM):

[source,bash]
----
oc get pods -n over-commit -l vm.kubevirt.io/name=overcommit-vm-1
NAME                                  READY   STATUS    RESTARTS   AGE
virt-launcher-overcommit-vm-1-rtspw   2/2     Running   0          47m
----

View the pod's CPU requests:

[source,bash]
----
oc get pod -n over-commit <pod_name> \
-o jsonpath='{.spec.containers[?(@.name=="compute")].resources}{"\n"}' | jq

{
  "limits": {
    "devices.kubevirt.io/kvm": "1",
    "devices.kubevirt.io/tun": "1",
    "devices.kubevirt.io/vhost-net": "1"
  },
  "requests": {
    "cpu": "4800m",
    "devices.kubevirt.io/kvm": "1",
    "devices.kubevirt.io/tun": "1",
    "devices.kubevirt.io/vhost-net": "1",
    "ephemeral-storage": "50M",
    "memory": "2696Mi"
  }
}
----

**Using the OpenShift Console:**
Navigate to Workloads → Pods and select the virt-launcher pod.

image::exercise2/pod-cpu-details.png[title="Confirm number of CPUs", link=self, window=blank, width=100%]

*Key observation*: A VM with 48 vCPUs only requests `4800m` (0.1 * 48 = 4.8 CPU) by default!



[#memory]
== Memory Overcommit

By design, Kubernetes, and hence OpenShift and OpenShift Virtualization, does not allow use of swap.

Memory oversubscription without use of swap is hazardous because if the amount of memory required by processes running on a node exceeds the amount of RAM available, processes will be killed. That's not desirable in any event, but particularly not for VMs where the workloads running on VMs will be taken down if the VMs are killed. 

OpenShift Virtualization has built the wasp-agent component to permit controlled use of swap with VMs.
The wasp-agent component documentation can be found here : https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/virtualization/postinstallation-configuration#virt-using-wasp-agent-to-configure-higher-vm-workload-density_virt-configuring-higher-vm-workload-density

We won't configure the wasp-agent component in this lab.

The `memoryOvercommitPercentage` parameter tells CNV how to scale the memory request that is made for each VM created. So when set to 100, CNV calculates its memory request based on the full declared memory size of the VM; when it's set to a higher value, CNV sets the request to a smaller value than that actually required by the VM, allowing overcommitting of memory. 

For example, with a VM with 16 GiB configured, if the overcommit percentage is set to its default value of 100, the memory request that CNV will apply to the pod is 16 GiB plus some extra for the overhead of the QEMU process running the VM. If it's set to 200, the request will be set to 8 GiB plus the overhead.


=== Instructions

[start=1]
. Memory requests with the default overcommit percentage


Observe the memory requests for the virt-launcher pod compared to the memory allocated to the VM :

image::exercise2/vm-memory-assigned.png[title="VM Memory Assigned", link=self, window=blank, width=100%]


[source,bash]
----
oc get vm  overcommit-vm-1 -o json | jq .spec.template.spec.domain.memory
{
  "guest": "2Gi"
}
----

image::exercise2/virt-launcher-memory-request.png[title="Virt-Launcher Memory Request", link=self, window=blank, width=100%]

[source,bash]
----
oc get pod -l vm.kubevirt.io/name=overcommit-vm-1 -o json | jq .items[0].spec.containers[0].resources.requests.memory
"2696Mi"
----

We can observe that the virt-launcher pod is requesting 2696Mi which is 2048Mi + some overhead.

NOTE: This overhead is necessary and expected - it ensures the VM has enough resources to run properly.


[start=2]
. Now let's enable the memory overcommit feature


Navigate to Virtualization → Overview → Settings → Cluster → General Settings and Enable "Enable memory density"

image::exercise2/enable-memory-overcommit.png[title="Enable Memory Overcommit", link=self, window=blank, width=100%]

You can confirm using the CLI :

[source,bash]
----
oc get hyperconverged -n openshift-cnv kubevirt-hyperconverged -o json | jq '.spec.higherWorkloadDensity.memoryOvercommitPercentage'
150
----

This means that the memory overcommit ratio is 150% which is 1.5:1.


[start=3]
. Now let's observe the memory requests for the virt-launcher pod compared to the memory allocated to the VM.

Live Migrate the VM (or stop and restart it) to apply the new setting, then observe the memory requests for the virt-launcher pod :


image::exercise2/virt-launcher-memory-request-overcommit.png[title="Virt-Launcher Memory Request 1.5x", link=self, window=blank, width=100%]


We can observe that the virt-launcher pod is requesting 2109734912 bytes which is ~ 2011Mi = ~1.96GiB.

This is less than the 2048MiB requested by the VM. This is because the memory overcommit ratio is 150%.


[start=4]
. Let's increase the memory overcommit ratio to 200%.



[source,bash]
----
oc patch hyperconverged -n openshift-cnv kubevirt-hyperconverged --type merge -p '{"spec":{"higherWorkloadDensity":{"memoryOvercommitPercentage":200}}}'
hyperconverged.hco.kubevirt.io/kubevirt-hyperconverged patched

oc get hyperconverged -n openshift-cnv kubevirt-hyperconverged -o json | jq '.spec.higherWorkloadDensity.memoryOvercommitPercentage'
200
----

This means that the memory overcommit ratio is now 200%.

Live migrate (or stop and restart) the VM to apply the new setting, then observe the memory requests for the virt-launcher pod from the web console or from the CLI:

[source,bash]
----
oc get pod -l vm.kubevirt.io/name=overcommit-vm-1 -o json | jq .items[0].spec.containers[0].resources.requests.memory
"1670Mi"
----

We can observe that the virt-launcher pod is now requesting 1670Mi which is ~ 1.6GiB.



