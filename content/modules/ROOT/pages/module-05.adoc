= Dynamic Rescheduling of Workloads


You can use the descheduler to evict pods based on specific strategies so that the pods can be rescheduled onto more appropriate nodes.

You can benefit from descheduling running pods in situations such as the following:

* Nodes are underutilized or overutilized.
* Pod and node affinity requirements, such as taints or labels, have changed and the original scheduling decisions are no longer appropriate for certain nodes.
* Node failure requires pods to be moved.
* New nodes are added to clusters.
* Pods have been restarted too many times.

== Objectives

In this module, we will guide you through the process of configuring Rescheduling Strategies with this profile:

* KubeVirtRelieveAndMigrate
  ** evicts pods from high-cost nodes to reduce overall resource expenses and enable workload migration. It also periodically rebalances workloads to help maintain similar spare capacity across nodes, which supports better handling of sudden workload spikes. Nodes can experience the following costs:

  *** Resource utilization: Increased resource pressure raises the overhead for running applications.
  *** Node maintenance: A higher number of containers on a node increases resource consumption and maintenance costs.

== Requirements

* Descheduler Operator must be installed
* The `KubeVirtRelieveAndMigrate` profile requires PSI metrics to be enabled on all worker nodes. For your convenience, this is already enabled in this lab.

== Instructions

=== 1. Access the LAB
1.1 Log in to the OpenShift cluster as the <admin> user with <password> as the password


[source,bash]
----
oc login -u admin -p redhatocp https://api.ocp4.example.com:6443
----


=== 2. Check which virtual machines are running in the `dynamic-schedule` namespace

[source,bash]
----
oc get vmi -n dynamic-schedule
----

You should see the following output:

[source,bash]
----
NAME     AGE     PHASE     IP             NODENAME
dynamic-schedule-vm1    10m     Running   10.129.2.111   worker01
dynamic-schedule-vm2    10m     Running   10.129.2.112   worker02
----

Or using the OpenShift Virtualization console, you can see the VMs in the `dynamic-schedule` namespace.

image::exercise5/05-image-vm-list.png[title="VM List", link=self, window=blank, width=100%]

=== 3. Configure the Descheduler Operator

[source,bash]
----
oc -n openshift-kube-descheduler-operator edit kubedescheduler cluster
----

You should see the following output:

[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: KubeDescheduler
metadata:
  name: cluster
  namespace: openshift-kube-descheduler-operator
spec:
  logLevel: Normal
  mode: Predictive
  operatorLogLevel: Normal
  profileCustomizations:
    devEnableSoftTainter: true
    devDeviationThresholds: AsymmetricLow
    devActualUtilizationProfile: PrometheusCPUCombined
    namespaces:
      included:
        - dynamic-schedule
  profiles:
    - DevKubeVirtRelieveAndMigrate # For OCP 4.20, use KubeVirtRelieveAndMigrate
  deschedulingIntervalSeconds: 3600 # This is the interval at which the descheduler will run ; Set a lower value for testing purposes
  managementState: Managed    
----

NOTE: Notice the mode is set to Predictive. By default, the descheduler does not evict pods. To evict pods, set mode to Automatic.


=== 4. Ensure the descheduler pod is running and check the logs in the openshift-kube-descheduler-operator namespace

[source,bash]
----
oc logs -n openshift-kube-descheduler-operator -l app=descheduler
----


=== 5. Deploy a stress test pod to validate the descheduler functionality

==== 5.1 Create a new project for the stress test
[source,bash]
----
oc new-project stress-test
----

==== 5.2 Create the configuration from a config map
[source,bash]
----
oc apply -f https://raw.githubusercontent.com/mmayeras/k8s-stress-test/refs/heads/main/stress-config.yaml
----

==== 5.3 Create the deployment
[source,bash]
----
https://raw.githubusercontent.com/mmayeras/k8s-stress-test/refs/heads/main/stress-deploy.yaml
----

==== 5.4 Edit the deployment to use a specific node selector

[source,bash]
----
oc patch deployment stress-test -p '{"spec":{"template":{"spec":{"nodeSelector":{"kubernetes.io/hostname":"worker01"}}}}}'
----

== 5.5 Verify the deployment
[source,bash]
----
oc get pods -n stress-test
----

==== 5.6 Observe the descheduler pod logs in the openshift-kube-descheduler-operator namespace
[source,bash]
----
oc logs -f -n openshift-kube-descheduler-operator -l app=descheduler
----

You should see logs like the following:

[source,text]
----
I1113 15:24:57.371043       1 lownodeutilization.go:261] "Number of overutilized nodes" totalNumber=1
I1113 15:24:57.371052       1 nodeutilization.go:174] "Total capacity to be moved" MetricResource=143
I1113 15:24:57.371273       1 nodeutilization.go:189] "Pods on node" node="worker01" allPods=72 nonRemovablePods=45 removablePods=27
I1113 15:30:00.673653       1 nodeutilization.go:205] "Evicting pods based on priority, if they have same priority, they'll be evicted based on QoS tiers"
I1113 15:30:00.673755       1 evictions.go:551] "Evicted pod in dry run mode" pod="dynamic-schedule/virt-launcher-vm01" reason="" strategy="LowNodeUtilization" node="worker01" profile="DevKubeVirtRelieveAndMigrate"
----


NOTE: Notice the mode is set to Predictive so the eviction is in dry run mode. To evict pods, set mode to Automatic.

=== 6. Turn on the descheduler mode to Automatic
[source,bash]
----
oc -n openshift-kube-descheduler-operator patch kubedescheduler cluster --type merge -p '{"spec":{"mode":"Automatic"}}'
----

=== 7. Verify the descheduler pod logs in the openshift-kube-descheduler-operator namespace
[source,bash]
----
oc logs -f -n openshift-kube-descheduler-operator -l app=descheduler
----

=== 8. Confirm the pods are evicted and rescheduled on another node

[source,bash]
----
oc get pods -n dynamic-schedule -o wide
----

You should see the following output: 

[source,text]
----
NAME                     READY   STATUS    RESTARTS   AGE     IP             NODE       NOMINATED NODE   READINESS GATES
virt-launcher-vm01       1/1     Running   0          10m     10.129.2.111   worker21   <none>           <none>
virt-launcher-vm02       1/1     Running   0          10m     10.129.2.112   worker02   <none>           <none>
----

